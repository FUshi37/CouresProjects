{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(128, 128)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x144 and 3136x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/yangzhe/Course_pra/RLRobot_Mase/DQL.ipynb 单元格 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangzhe/Course_pra/RLRobot_Mase/DQL.ipynb#W0sZmlsZQ%3D%3D?line=226'>227</a>\u001b[0m \u001b[39m# myrobot = MinDQNRobot(g)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangzhe/Course_pra/RLRobot_Mase/DQL.ipynb#W0sZmlsZQ%3D%3D?line=227'>228</a>\u001b[0m \u001b[39m# myrobot.memory.build_full_view(g)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangzhe/Course_pra/RLRobot_Mase/DQL.ipynb#W0sZmlsZQ%3D%3D?line=228'>229</a>\u001b[0m runner \u001b[39m=\u001b[39m Runner(myrobot)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/yangzhe/Course_pra/RLRobot_Mase/DQL.ipynb#W0sZmlsZQ%3D%3D?line=229'>230</a>\u001b[0m runner\u001b[39m.\u001b[39;49mrun_training(epoch, training_per_epoch)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangzhe/Course_pra/RLRobot_Mase/DQL.ipynb#W0sZmlsZQ%3D%3D?line=231'>232</a>\u001b[0m \u001b[39m# # 生成训练过程的gif图, 建议下载到本地查看；也可以注释该行代码，加快运行速度。\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangzhe/Course_pra/RLRobot_Mase/DQL.ipynb#W0sZmlsZQ%3D%3D?line=232'>233</a>\u001b[0m \u001b[39m# runner.generate_gif(filename=\"results/dqn_size10.gif\")\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangzhe/Course_pra/RLRobot_Mase/DQL.ipynb#W0sZmlsZQ%3D%3D?line=234'>235</a>\u001b[0m \u001b[39m\"\"\"Test Robot\"\"\"\u001b[39;00m    \n",
      "File \u001b[0;32m~/Course_pra/RLRobot_Mase/Runner.py:55\u001b[0m, in \u001b[0;36mRunner.run_training\u001b[0;34m(self, training_epoch, training_per_epoch)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_robot_record\u001b[39m.\u001b[39mappend(current_record)\n\u001b[1;32m     53\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m action, reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrobot\u001b[39m.\u001b[39;49mtrain_update()\n\u001b[1;32m     56\u001b[0m current_record[\u001b[39m'\u001b[39m\u001b[39maction\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m action\n\u001b[1;32m     57\u001b[0m current_record[\u001b[39m'\u001b[39m\u001b[39mreward\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m reward\n",
      "\u001b[1;32m/home/yangzhe/Course_pra/RLRobot_Mase/DQL.ipynb 单元格 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangzhe/Course_pra/RLRobot_Mase/DQL.ipynb#W0sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m state \u001b[39m=\u001b[39m (state_image, state_info)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangzhe/Course_pra/RLRobot_Mase/DQL.ipynb#W0sZmlsZQ%3D%3D?line=165'>166</a>\u001b[0m \u001b[39m# 根据当前状态选择动作\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/yangzhe/Course_pra/RLRobot_Mase/DQL.ipynb#W0sZmlsZQ%3D%3D?line=166'>167</a>\u001b[0m action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_choose_action(state)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangzhe/Course_pra/RLRobot_Mase/DQL.ipynb#W0sZmlsZQ%3D%3D?line=168'>169</a>\u001b[0m \u001b[39m# 执行动作并获取奖励\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangzhe/Course_pra/RLRobot_Mase/DQL.ipynb#W0sZmlsZQ%3D%3D?line=169'>170</a>\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaze\u001b[39m.\u001b[39mmove_robot(action)\n",
      "\u001b[1;32m/home/yangzhe/Course_pra/RLRobot_Mase/DQL.ipynb 单元格 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangzhe/Course_pra/RLRobot_Mase/DQL.ipynb#W0sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_model\u001b[39m.\u001b[39meval()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangzhe/Course_pra/RLRobot_Mase/DQL.ipynb#W0sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/yangzhe/Course_pra/RLRobot_Mase/DQL.ipynb#W0sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m     q_next \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_model(state_image)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnumpy()  \u001b[39m# use target model choose action\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangzhe/Course_pra/RLRobot_Mase/DQL.ipynb#W0sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangzhe/Course_pra/RLRobot_Mase/DQL.ipynb#W0sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_action[np\u001b[39m.\u001b[39margmin(q_next)\u001b[39m.\u001b[39mitem()]\n",
      "File \u001b[0;32m~/anaconda3/envs/mv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Course_pra/RLRobot_Mase/torch_py/QNetworknew.py:35\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     32\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[39m# 过全连接层\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x))\n\u001b[1;32m     36\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)\n\u001b[1;32m     37\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/mv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/mv/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x144 and 3136x512)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from QRobot import QRobot\n",
    "from Maze import Maze\n",
    "from ReplayDataSet import ReplayDataSet\n",
    "from torch_py.QNetworknew import QNetwork\n",
    "import cv2\n",
    "\n",
    "class Robot(QRobot):\n",
    "    valid_action = ['u', 'r', 'd', 'l']\n",
    "\n",
    "    ''' QLearning parameters'''\n",
    "    epsilon = 1  # 初始贪心算法探索概率\n",
    "    gamma = 0.9  # 公式中的 γ\n",
    "    epsilon_decay = 0.98  # 每次更新衰减率\n",
    "\n",
    "    EveryUpdate = 1  # the interval of target model's updating\n",
    "    EveryUpdateTarget = 10\n",
    "    \n",
    "    learning_rate = 2e-2\n",
    "    \"\"\"some parameters of neural network\"\"\"\n",
    "    target_model = None\n",
    "    eval_model = None\n",
    "    batch_size = 32\n",
    "    # learning_rate = 2e-2\n",
    "    TAU = 1e-3\n",
    "    step = 1  # 记录训练的步数\n",
    "\n",
    "    \"\"\"setting the device to train network\"\"\"\n",
    "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    def __init__(self, maze):\n",
    "        \"\"\"\n",
    "        初始化 Robot 类\n",
    "        :param maze:迷宫对象\n",
    "        \"\"\"\n",
    "        super(Robot, self).__init__(maze)\n",
    "        maze.set_reward(reward={\n",
    "            \"hit_wall\": 10.,\n",
    "            \"destination\": -maze.maze_size**2 * 3.,\n",
    "            \"default\": 1,\n",
    "        })\n",
    "        self.maze = maze\n",
    "        self.maze_size = maze.maze_size\n",
    "        \n",
    "        self.epsilon = self.epsilon\n",
    "        self.gamma = self.gamma\n",
    "        \n",
    "        self.epsilon_decay = self.epsilon_decay\n",
    "        self.batch_size = pow(2, int(self.maze_size/2 + 3)) \n",
    "        if self.batch_size > 256:\n",
    "            self.batch_size = 256\n",
    "        self.learning_rate = self.learning_rate / self.maze_size \n",
    "        \n",
    "\n",
    "        \"\"\"build network\"\"\"\n",
    "        self.target_model = None\n",
    "        self.eval_model = None\n",
    "        self._build_network()\n",
    "\n",
    "        \"\"\"create the memory to store data\"\"\"\n",
    "        max_size = max(self.maze_size ** 2 * 3, 1e4)\n",
    "        self.memory = ReplayDataSet(max_size=max_size)\n",
    "        self.memory.build_full_view(maze)\n",
    "\n",
    "    def _build_network(self):\n",
    "        seed = 0\n",
    "        random.seed(seed)\n",
    "        input_channels = 1\n",
    "        action_size = len(self.valid_action)\n",
    "        \"\"\"build target model\"\"\"\n",
    "        # self.target_model = QNetwork(state_size=2, action_size=4, seed=seed).to(self.device)\n",
    "        self.target_model = QNetwork(input_channels=input_channels, action_size=action_size, seed=seed).to(self.device)\n",
    "        \"\"\"build eval model\"\"\"\n",
    "        # self.eval_model = QNetwork(state_size=2, action_size=4, seed=seed).to(self.device)\n",
    "        self.eval_model = QNetwork(input_channels=input_channels, action_size=action_size, seed=seed).to(self.device)\n",
    "\n",
    "        \"\"\"build the optimizer\"\"\"\n",
    "        self.optimizer = optim.Adam(self.eval_model.parameters(), lr=self.learning_rate)\n",
    "        # self.optimizer = optim.SGD(self.eval_model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def target_replace_op(self):\n",
    "        \"\"\"\n",
    "            Soft update the target model parameters.\n",
    "            θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        \"\"\"\n",
    "\n",
    "        # for target_param, eval_param in zip(self.target_model.parameters(), self.eval_model.parameters()):\n",
    "        #     target_param.data.copy_(self.TAU * eval_param.data + (1.0 - self.TAU) * target_param.data)\n",
    "\n",
    "        \"\"\" replace the whole parameters\"\"\"\n",
    "        self.target_model.load_state_dict(self.eval_model.state_dict())\n",
    "\n",
    "    def _choose_action(self, state):\n",
    "        state_image, state_info = state\n",
    "        state_image = torch.from_numpy(state_image).float().unsqueeze(0).to(self.device)\n",
    "        # 这里的实现需要根据状态信息来选择动作\n",
    "        # 然后根据 state_image 选择动作\n",
    "        # 下面是一个示例实现，你需要根据具体情况进行调整\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice(self.valid_action)\n",
    "        else:\n",
    "            self.eval_model.eval()\n",
    "            with torch.no_grad():\n",
    "                q_next = self.eval_model(state_image).cpu().data.numpy()  # use target model choose action\n",
    "            self.eval_model.train()\n",
    "            action = self.valid_action[np.argmin(q_next).item()]\n",
    "        return action\n",
    "\n",
    "\n",
    "    def _learn(self, batch: int = 16):\n",
    "        if len(self.memory) < batch:\n",
    "            print(\"the memory data is not enough\")\n",
    "            return\n",
    "        state, action_index, reward, next_state, is_terminal = self.memory.random_sample(batch)\n",
    "\n",
    "        \"\"\" convert the data to tensor type\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(self.device)\n",
    "        action_index = torch.from_numpy(action_index).long().to(self.device)\n",
    "        reward = torch.from_numpy(reward).float().to(self.device)\n",
    "        next_state = torch.from_numpy(next_state).float().to(self.device)\n",
    "        is_terminal = torch.from_numpy(is_terminal).int().to(self.device)\n",
    "\n",
    "        self.eval_model.train()\n",
    "        self.target_model.eval()\n",
    "\n",
    "        \"\"\"Get max predicted Q values (for next states) from target model\"\"\"\n",
    "        Q_targets_next = self.target_model(next_state).detach().min(1)[0].unsqueeze(1)\n",
    "\n",
    "        \"\"\"Compute Q targets for current states\"\"\"\n",
    "        Q_targets = reward + self.gamma * Q_targets_next * (torch.ones_like(is_terminal) - is_terminal)\n",
    "\n",
    "        \"\"\"Get expected Q values from local model\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        Q_expected = self.eval_model(state).gather(dim=1, index=action_index)\n",
    "\n",
    "        \"\"\"Compute loss\"\"\"\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        loss_item = loss.item()\n",
    "\n",
    "        \"\"\" Minimize the loss\"\"\"\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        \"\"\"copy the weights of eval_model to the target_model\"\"\"\n",
    "        if self.step % (self.EveryUpdateTarget) == 0:\n",
    "            self.target_replace_op()\n",
    "        return loss_item\n",
    "\n",
    "    def train_update(self):\n",
    "        # 获取迷宫状态的图像表示\n",
    "        state_image = self.maze.generate_state_representation()\n",
    "        print(type(state_image))\n",
    "        print(state_image.shape)\n",
    "        # 使用 QRobot 类的 sense_state 方法获取当前状态的其他信息\n",
    "        state_info = self.sense_state()\n",
    "        \n",
    "        # 将图像表示和其他状态信息组合成一个状态张量\n",
    "        state = (state_image, state_info)\n",
    "        \n",
    "        # 根据当前状态选择动作\n",
    "        action = self._choose_action(state)\n",
    "        \n",
    "        # 执行动作并获取奖励\n",
    "        reward = self.maze.move_robot(action)\n",
    "        \n",
    "        # 获取下一个状态的图像表示\n",
    "        next_state_image = self.maze.generate_state_representation()\n",
    "        \n",
    "        # 使用 QRobot 类的 sense_state 方法获取下一个状态的其他信息\n",
    "        next_state_info = self.sense_state()\n",
    "        \n",
    "        # 将图像表示和其他状态信息组合成一个下一个状态张量\n",
    "        next_state = (next_state_image, next_state_info)\n",
    "        \n",
    "        # 判断是否达到终点\n",
    "        is_terminal = 1 if next_state_image == self.maze.destination or next_state_image == state_image else 0\n",
    "\n",
    "        # 将状态、动作、奖励、下一个状态和终止标志添加到记忆中\n",
    "        self.memory.add(state, self.valid_action.index(action), reward, next_state, is_terminal)\n",
    "        \n",
    "        # 间隔一段时间更新 target network 权重\n",
    "        if self.step % self.EveryUpdate == 0:\n",
    "            loss = self._learn(batch=self.batch_size)\n",
    "        \n",
    "        # 更新步数和 epsilon\n",
    "        self.step += 1\n",
    "        self.epsilon = max(0.01, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        return action, reward\n",
    "\n",
    "\n",
    "    def test_update(self):\n",
    "        # state = np.array(self.sense_state(), dtype=int)\n",
    "        state = self.maze.generate_state_representation()\n",
    "        state = torch.from_numpy(state).float().to(self.device)\n",
    "\n",
    "        self.eval_model.eval()\n",
    "        with torch.no_grad():\n",
    "            q_value = self.eval_model(state).cpu().data.numpy()\n",
    "\n",
    "        action = self.valid_action[np.argmin(q_value).item()]\n",
    "        reward = self.maze.move_robot(action)\n",
    "        return action, reward\n",
    "\n",
    "\n",
    "from QRobot import QRobot\n",
    "from Maze import Maze\n",
    "from Runner import Runner\n",
    "from torch_py.MinDQNRobot import MinDQNRobot\n",
    "\n",
    "\"\"\"  Deep Qlearning 算法相关参数： \"\"\"\n",
    "\n",
    "epoch = 100  # 训练轮数\n",
    "maze_size = 5 # 迷宫size\n",
    "training_per_epoch=int(maze_size * maze_size * 3)\n",
    "\n",
    "\"\"\" 使用 DQN 算法训练 \"\"\"\n",
    "\n",
    "g = Maze(maze_size=maze_size)\n",
    "myrobot = Robot(g)\n",
    "# myrobot = MinDQNRobot(g)\n",
    "# myrobot.memory.build_full_view(g)\n",
    "runner = Runner(myrobot)\n",
    "runner.run_training(epoch, training_per_epoch)\n",
    "\n",
    "# # 生成训练过程的gif图, 建议下载到本地查看；也可以注释该行代码，加快运行速度。\n",
    "# runner.generate_gif(filename=\"results/dqn_size10.gif\")\n",
    "\n",
    "\"\"\"Test Robot\"\"\"    \n",
    "myrobot.reset()\n",
    "for _ in range(25):\n",
    "    a, r = myrobot.test_update()\n",
    "    loc = myrobot.maze.robot['loc']\n",
    "    print(\"action:%s, reward:%.2f, loc:%s\" % (a, r, loc))\n",
    "    if myrobot.maze.robot['loc'] == myrobot.maze.destination:\n",
    "        print(\"success\")\n",
    "        break\n",
    "\n",
    "print(g)\n",
    "runner.plot_results()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
